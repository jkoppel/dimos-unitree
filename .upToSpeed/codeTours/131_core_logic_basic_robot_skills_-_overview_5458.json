{
  "title": "13.1: Core Logic: Basic Robot Skills - Overview",
  "id": "kPSW65iXWwqhXPASqO3FD2yndkZtxVLb39RNH95YWYY=",
  "originalId": 5458,
  "position": 45,
  "steps": [
    {
      "type": "textOnly",
      "description": "This tour will guide you through the files in the `dimos/skills/` directory, showing their purpose and main classes.",
      "title": "",
      "id": "68673"
    },
    {
      "type": "textOnly",
      "description": "Directory contents: `kill_skill.py`, `navigation.py`, `observe_stream.py`, `speak.py`, `visual_navigation_skills.py`",
      "title": "",
      "id": "68674"
    },
    {
      "type": "revealFiles",
      "files": [
        "dimos/skills/kill_skill.py"
      ],
      "description": "Let's start with `kill_skill.py`: a skill for terminating other running skills.",
      "title": "",
      "id": "68675",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "The **module docstring** explains that this skill terminates other running skills, particularly background or threaded ones.",
      "file": "dimos/skills/kill_skill.py",
      "highlight": [
        {
          "start": 15,
          "end": 20
        }
      ],
      "title": "",
      "id": "68676",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "The `KillSkill` class (line 31) inherits `AbstractSkill` and uses `SkillLibrary` to terminate named skills.",
      "file": "dimos/skills/kill_skill.py",
      "highlight": [
        {
          "start": 31,
          "end": 31
        }
      ],
      "title": "",
      "id": "68677",
      "hideAreas": []
    },
    {
      "type": "revealFiles",
      "files": [
        "dimos/skills/navigation.py"
      ],
      "description": "Next, `navigation.py` provides semantic map and navigation skills.",
      "title": "",
      "id": "68678",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "This docstring (lines 15–21) outlines two skills: building a semantic map and querying it for navigation.",
      "file": "dimos/skills/navigation.py",
      "highlight": [
        {
          "start": 15,
          "end": 21
        }
      ],
      "title": "",
      "id": "68679",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "`NavigateWithText` (line 66) tries vision-based object navigation first, then falls back to semantic-map queries.",
      "file": "dimos/skills/navigation.py",
      "highlight": [
        {
          "start": 66,
          "end": 66
        }
      ],
      "title": "",
      "id": "68680",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "`GetPose` (line 384) returns the robot’s current pose and can save named locations into spatial memory.",
      "file": "dimos/skills/navigation.py",
      "highlight": [
        {
          "start": 384,
          "end": 384
        }
      ],
      "title": "",
      "id": "68681",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "`NavigateToGoal` (line 470) sets a global planner goal and follows it locally, supporting safe cancellation.",
      "file": "dimos/skills/navigation.py",
      "highlight": [
        {
          "start": 470,
          "end": 470
        }
      ],
      "title": "",
      "id": "68682",
      "hideAreas": []
    },
    {
      "type": "mcq",
      "description": "A user commands the robot, `Go to the kitchen table,` which is not currently in view. According to the `NavigateWithText` skill's design, what is the expected sequence of actions?\n\nOptions:\n\n A). The skill will fail, returning an error that the object is not in the camera's view.\n\nB). The skill will first perform a visual search; when that fails, it will query the spatial memory for the location.\n\nC). The skill will query the spatial memory directly, bypassing the visual search since the target is a location.\n\nD). The skill will activate `ObserveStream` to continuously scan the area until the table is found.\n\n\nCorrect: B). The skill will first perform a visual search; when that fails, it will query the spatial memory for the location.\n\nExplanation: The `NavigateWithText` skill is designed with a two-step logic, as described in its `__call__` method. It first attempts to locate the object visually (`_navigate_to_object`). If that fails, it falls back to querying the semantic map (`_navigate_using_semantic_map`). Failing immediately ignores the fallback mechanism. Querying the map directly ignores the primary visual search. Using `ObserveStream` confuses a continuous monitoring skill with a direct navigation command.",
      "title": "",
      "id": "68692",
      "text": "A user commands the robot, `Go to the kitchen table,` which is not currently in view. According to the `NavigateWithText` skill's design, what is the expected sequence of actions?",
      "answers": [
        "The skill will fail, returning an error that the object is not in the camera's view.",
        "The skill will first perform a visual search; when that fails, it will query the spatial memory for the location.",
        "The skill will query the spatial memory directly, bypassing the visual search since the target is a location.",
        "The skill will activate `ObserveStream` to continuously scan the area until the table is found."
      ],
      "correct": 1,
      "explanation": "The `NavigateWithText` skill is designed with a two-step logic, as described in its `__call__` method. It first attempts to locate the object visually (`_navigate_to_object`). If that fails, it falls back to querying the semantic map (`_navigate_using_semantic_map`). Failing immediately ignores the fallback mechanism. Querying the map directly ignores the primary visual search. Using `ObserveStream` confuses a continuous monitoring skill with a direct navigation command."
    },
    {
      "type": "revealFiles",
      "files": [
        "dimos/skills/observe_stream.py"
      ],
      "description": "Now, `observe_stream.py` defines a continuous-perception skill.",
      "title": "",
      "id": "68683",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "Docstring (lines 15–20) explains periodic image capture from the `ROS` video stream for **agent inference**.",
      "file": "dimos/skills/observe_stream.py",
      "highlight": [
        {
          "start": 15,
          "end": 20
        }
      ],
      "title": "",
      "id": "68684",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "`ObserveStream` (line 39) uses `RxPY`’s interval observable with a threadpool scheduler for non-blocking monitoring.",
      "file": "dimos/skills/observe_stream.py",
      "highlight": [
        {
          "start": 39,
          "end": 39
        }
      ],
      "title": "",
      "id": "68685",
      "hideAreas": []
    },
    {
      "type": "revealFiles",
      "files": [
        "dimos/skills/speak.py"
      ],
      "description": "Next, `speak.py` handles text-to-speech for robot communication.",
      "title": "",
      "id": "68686",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "`Speak` (lines 64–68) enqueues TTS tasks in a global queue (`RLock`-protected) and applies dynamic timeouts based on text length.",
      "file": "dimos/skills/speak.py",
      "highlight": [
        {
          "start": 64,
          "end": 68
        }
      ],
      "title": "",
      "id": "68687",
      "hideAreas": []
    },
    {
      "type": "revealFiles",
      "files": [
        "dimos/skills/visual_navigation_skills.py"
      ],
      "description": "Finally, `visual_navigation_skills.py` provides computer-vision navigation.",
      "title": "",
      "id": "68688",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "**Docstring (lines 15–20)** describes skills for following humans and object-based navigation using vision.",
      "file": "dimos/skills/visual_navigation_skills.py",
      "highlight": [
        {
          "start": 15,
          "end": 20
        }
      ],
      "title": "",
      "id": "68689",
      "hideAreas": []
    },
    {
      "type": "highlight",
      "description": "`FollowHuman` (line 34) uses `VisualServoing` to maintain a safe distance from a tracked person until timeout or stop.",
      "file": "dimos/skills/visual_navigation_skills.py",
      "highlight": [
        {
          "start": 34,
          "end": 34
        }
      ],
      "title": "",
      "id": "68690",
      "hideAreas": []
    },
    {
      "type": "textOnly",
      "description": "This completes our walkthrough. You now understand each `skill file`’s role and main classes.",
      "title": "",
      "id": "68691"
    }
  ]
}